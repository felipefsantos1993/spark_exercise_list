{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88995c26",
   "metadata": {},
   "source": [
    "# PySpark ‚Äî Lista de Exerc√≠cios (115+)\n",
    "\n",
    "Este notebook cont√©m 115 exerc√≠cios organizados por n√≠vel (B√°sico, Intermedi√°rio, Avan√ßado). Cada exerc√≠cio traz um enunciado e um c√≥digo inicial para voc√™ completar e rodar no seu ambiente (Databricks, local, EMR etc.).\n",
    "\n",
    "> Instru√ß√µes: Para cada exerc√≠cio, siga o enunciado, complete o c√≥digo e execute a c√©lula. Coment√°rios e dicas est√£o inclu√≠dos em cada bloco de c√≥digo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40860564",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üü¶ Se√ß√£o: B√°sico\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7aacf2",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 1: Instalar e configurar o PySpark no ambiente local.\n",
    "\n",
    "**Descri√ß√£o:** Objetivo: Instalar e configurar o PySpark no ambiente local.\n",
    "\n",
    "Instru√ß√µes: Implemente o c√≥digo usando PySpark, seguindo boas pr√°ticas.\n",
    "\n",
    "**Starter code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3fbbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo inicial (comente e complete)\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# TODO: completar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67888ded",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 2: Criar uma SparkSession simples.\n",
    "\n",
    "**Descri√ß√£o:** Objetivo: Criar uma SparkSession simples.\n",
    "\n",
    "Instru√ß√µes: Implemente o c√≥digo usando PySpark, seguindo boas pr√°ticas.\n",
    "\n",
    "**Starter code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685c72f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo inicial (comente e complete)\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# TODO: completar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b745d0d",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 3: Ler um arquivo CSV local e exibir os primeiros registros.\n",
    "\n",
    "**Descri√ß√£o:** Objetivo: Ler um arquivo CSV local e exibir os primeiros registros.\n",
    "\n",
    "Instru√ß√µes: Implemente o c√≥digo usando PySpark, seguindo boas pr√°ticas.\n",
    "\n",
    "**Starter code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255eb904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo inicial (comente e complete)\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# TODO: completar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4995707",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 4: Ler um arquivo Parquet e mostrar o schema.\n",
    "\n",
    "**Descri√ß√£o:** Objetivo: Ler um arquivo Parquet e mostrar o schema.\n",
    "\n",
    "Instru√ß√µes: Implemente o c√≥digo usando PySpark, seguindo boas pr√°ticas.\n",
    "\n",
    "**Starter code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9564c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo inicial (comente e complete)\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# TODO: completar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef20b341",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 5: Criar um DataFrame manualmente a partir de uma lista de tuplas.\n",
    "\n",
    "**Descri√ß√£o:** Objetivo: Criar um DataFrame manualmente a partir de uma lista de tuplas.\n",
    "\n",
    "Instru√ß√µes: Implemente o c√≥digo usando PySpark, seguindo boas pr√°ticas.\n",
    "\n",
    "**Starter code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126c7de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo inicial (comente e complete)\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# TODO: completar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c992df4",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 6: Definir o schema manualmente ao criar um DataFrame.\n",
    "\n",
    "**Descri√ß√£o:** Objetivo: Definir o schema manualmente ao criar um DataFrame.\n",
    "\n",
    "Instru√ß√µes: Implemente o c√≥digo usando PySpark, seguindo boas pr√°ticas.\n",
    "\n",
    "**Starter code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea36aa0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo inicial (comente e complete)\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# TODO: completar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7910a8",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 7: Mostrar o n√∫mero total de linhas de um DataFrame (count).\n",
    "\n",
    "**Descri√ß√£o:** Objetivo: Mostrar o n√∫mero total de linhas de um DataFrame (count).\n",
    "\n",
    "Instru√ß√µes: Implemente o c√≥digo usando PySpark, seguindo boas pr√°ticas.\n",
    "\n",
    "**Starter code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d21a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo inicial (comente e complete)\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# TODO: completar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f242fded",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 8: Selecionar colunas espec√≠ficas (select).\n",
    "\n",
    "**Descri√ß√£o:** Objetivo: Selecionar colunas espec√≠ficas (select).\n",
    "\n",
    "Instru√ß√µes: Implemente o c√≥digo usando PySpark, seguindo boas pr√°ticas.\n",
    "\n",
    "**Starter code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc54f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo inicial (comente e complete)\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# TODO: completar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c72af7",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 9: Renomear colunas.\n",
    "\n",
    "**Descri√ß√£o:** Objetivo: Renomear colunas.\n",
    "\n",
    "Instru√ß√µes: Implemente o c√≥digo usando PySpark, seguindo boas pr√°ticas.\n",
    "\n",
    "**Starter code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c33e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo inicial (comente e complete)\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# TODO: completar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec98f75",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 10: Filtrar linhas com valores espec√≠ficos (filter).\n",
    "\n",
    "**Descri√ß√£o:** Objetivo: Filtrar linhas com valores espec√≠ficos (filter).\n",
    "\n",
    "Instru√ß√µes: Implemente o c√≥digo usando PySpark, seguindo boas pr√°ticas.\n",
    "\n",
    "**Starter code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96e03f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo inicial (comente e complete)\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# TODO: completar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacea404",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 11: Usar operadores l√≥gicos (and, or, not) em filtros.\n",
    "\n",
    "**Descri√ß√£o:** Objetivo: Usar operadores l√≥gicos (and, or, not) em filtros.\n",
    "\n",
    "Instru√ß√µes: Implemente o c√≥digo usando PySpark, seguindo boas pr√°ticas.\n",
    "\n",
    "**Starter code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3f65ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo inicial (comente e complete)\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# TODO: completar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3a201e",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 12: Ordenar dados (orderBy).\n",
    "\n",
    "**Descri√ß√£o:** Objetivo: Ordenar dados (orderBy).\n",
    "\n",
    "Instru√ß√µes: Implemente o c√≥digo usando PySpark, seguindo boas pr√°ticas.\n",
    "\n",
    "**Starter code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554f8606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo inicial (comente e complete)\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# TODO: completar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1611dab2",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 13: Criar colunas derivadas (withColumn).\n",
    "\n",
    "**Descri√ß√£o:** Objetivo: Criar colunas derivadas (withColumn).\n",
    "\n",
    "Instru√ß√µes: Implemente o c√≥digo usando PySpark, seguindo boas pr√°ticas.\n",
    "\n",
    "**Starter code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc351166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo inicial (comente e complete)\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# TODO: completar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440b2255",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 14: Remover colunas (drop).\n",
    "\n",
    "**Descri√ß√£o:** Objetivo: Remover colunas (drop).\n",
    "\n",
    "Instru√ß√µes: Implemente o c√≥digo usando PySpark, seguindo boas pr√°ticas.\n",
    "\n",
    "**Starter code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d781b465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo inicial (comente e complete)\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# TODO: completar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0136542",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 15: Converter tipos de colunas (cast).\n",
    "\n",
    "**Descri√ß√£o:** Objetivo: Converter tipos de colunas (cast).\n",
    "\n",
    "Instru√ß√µes: Implemente o c√≥digo usando PySpark, seguindo boas pr√°ticas.\n",
    "\n",
    "**Starter code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d84fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo inicial (comente e complete)\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# TODO: completar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068871e4",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 16: Exibir valores √∫nicos de uma coluna (distinct).\n",
    "\n",
    "**Descri√ß√£o:** Objetivo: Exibir valores √∫nicos de uma coluna (distinct).\n",
    "\n",
    "Instru√ß√µes: Implemente o c√≥digo usando PySpark, seguindo boas pr√°ticas.\n",
    "\n",
    "**Starter code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf0e08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo inicial (comente e complete)\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# TODO: completar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31196de",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 17: Contar valores √∫nicos (distinct().count()).\n",
    "\n",
    "**Descri√ß√£o:** Objetivo: Contar valores √∫nicos (distinct().count()).\n",
    "\n",
    "Instru√ß√µes: Implemente o c√≥digo usando PySpark, seguindo boas pr√°ticas.\n",
    "\n",
    "**Starter code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53d4419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo inicial (comente e complete)\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# TODO: completar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246147ae",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 18: Filtrar valores nulos (filter + isNull).\n",
    "\n",
    "**Descri√ß√£o:** Objetivo: Filtrar valores nulos (filter + isNull).\n",
    "\n",
    "Instru√ß√µes: Implemente o c√≥digo usando PySpark, seguindo boas pr√°ticas.\n",
    "\n",
    "**Starter code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf58d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo inicial (comente e complete)\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# TODO: completar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b2c372",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 19: Substituir valores nulos (fillna).\n",
    "\n",
    "**Descri√ß√£o:** Objetivo: Substituir valores nulos (fillna).\n",
    "\n",
    "Instru√ß√µes: Implemente o c√≥digo usando PySpark, seguindo boas pr√°ticas.\n",
    "\n",
    "**Starter code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85aa6887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo inicial (comente e complete)\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# TODO: completar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a95c193",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 20: Ler um JSON local e visualizar dados.\n",
    "\n",
    "**Descri√ß√£o:** Objetivo: Ler um JSON local e visualizar dados.\n",
    "\n",
    "Instru√ß√µes: Implemente o c√≥digo usando PySpark, seguindo boas pr√°ticas.\n",
    "\n",
    "**Starter code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33867fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo inicial (comente e complete)\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# TODO: completar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c37ab0",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 21: Salvar um DataFrame em formato CSV.\n",
    "\n",
    "**Descri√ß√£o:** Objetivo: Salvar um DataFrame em formato CSV.\n",
    "\n",
    "Instru√ß√µes: Implemente o c√≥digo usando PySpark, seguindo boas pr√°ticas.\n",
    "\n",
    "**Starter code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa49b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo inicial (comente e complete)\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# TODO: completar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62fbe85a",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 22: Salvar um DataFrame em formato Parquet.\n",
    "\n",
    "**Descri√ß√£o:** Objetivo: Salvar um DataFrame em formato Parquet.\n",
    "\n",
    "Instru√ß√µes: Implemente o c√≥digo usando PySpark, seguindo boas pr√°ticas.\n",
    "\n",
    "**Starter code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5abaa1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo inicial (comente e complete)\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# TODO: completar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9648bd8c",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 23: Salvar dados em JSON.\n",
    "\n",
    "**Descri√ß√£o:** Objetivo: Salvar dados em JSON.\n",
    "\n",
    "Instru√ß√µes: Implemente o c√≥digo usando PySpark, seguindo boas pr√°ticas.\n",
    "\n",
    "**Starter code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3a147c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo inicial (comente e complete)\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# TODO: completar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558af86c",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 24: Usar .describe() para estat√≠sticas descritivas.\n",
    "\n",
    "**Descri√ß√£o:** Objetivo: Usar .describe() para estat√≠sticas descritivas.\n",
    "\n",
    "Instru√ß√µes: Implemente o c√≥digo usando PySpark, seguindo boas pr√°ticas.\n",
    "\n",
    "**Starter code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff8897c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo inicial (comente e complete)\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# TODO: completar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac626794",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 25: Criar colunas a partir de express√µes (expr).\n",
    "\n",
    "**Descri√ß√£o:** Objetivo: Criar colunas a partir de express√µes (expr).\n",
    "\n",
    "Instru√ß√µes: Implemente o c√≥digo usando PySpark, seguindo boas pr√°ticas.\n",
    "\n",
    "**Starter code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655798dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo inicial (comente e complete)\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# TODO: completar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d04133b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üü¶ Se√ß√£o: Intermedi√°rio\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4fd56b",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 26: Agrupar dados e calcular m√©dias (groupBy().avg()).\n",
    "\n",
    "**Descri√ß√£o:** Objetivo: Agrupar dados e calcular m√©dias (groupBy().avg()).\n",
    "\n",
    "Instru√ß√µes: implemente usando fun√ß√µes do PySpark.\n",
    "\n",
    "**Starter code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f706486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo inicial\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# TODO: completar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a90b3c3",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 27: Calcular soma, m√≠nimo e m√°ximo por grupo (groupBy().agg()).\n",
    "\n",
    "**Descri√ß√£o:** Objetivo: Calcular soma, m√≠nimo e m√°ximo por grupo (groupBy().agg()).\n",
    "\n",
    "Instru√ß√µes: implemente usando fun√ß√µes do PySpark.\n",
    "\n",
    "**Starter code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2d25fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo inicial\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# TODO: completar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7bfe54d",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 28: Contar registros por categoria.\n",
    "\n",
    "**Descri√ß√£o:** Objetivo: Contar registros por categoria.\n",
    "\n",
    "Instru√ß√µes: implemente usando fun√ß√µes do PySpark.\n",
    "\n",
    "**Starter code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80de639f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo inicial\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# TODO: completar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4ec675",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 29: Usar fun√ß√µes de janela (Window).\n",
    "\n",
    "**Descri√ß√£o:** Objetivo: Usar fun√ß√µes de janela (Window).\n",
    "\n",
    "Instru√ß√µes: implemente usando fun√ß√µes do PySpark.\n",
    "\n",
    "**Starter code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6179d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo inicial\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# TODO: completar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63b5f2a",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 30: Criar coluna com rank (row_number).\n",
    "\n",
    "**Descri√ß√£o:** Objetivo: Criar coluna com rank (row_number).\n",
    "\n",
    "Instru√ß√µes: implemente usando fun√ß√µes do PySpark.\n",
    "\n",
    "**Starter code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0999b941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo inicial\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# TODO: completar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed57fd29",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 31: Calcular m√©dia m√≥vel usando janela (avg().over()).\n",
    "\n",
    "**Descri√ß√£o:** Objetivo: Calcular m√©dia m√≥vel usando janela (avg().over()).\n",
    "\n",
    "Instru√ß√µes: implemente usando fun√ß√µes do PySpark.\n",
    "\n",
    "**Starter code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f77c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo inicial\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# TODO: completar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70992776",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 32: Realizar join interno (inner join).\n",
    "\n",
    "**Descri√ß√£o:** Objetivo: Realizar join interno (inner join).\n",
    "\n",
    "Instru√ß√µes: implemente usando fun√ß√µes do PySpark.\n",
    "\n",
    "**Starter code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f3894a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo inicial\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# TODO: completar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e407324a",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 33: Realizar join externo (outer join).\n",
    "\n",
    "**Descri√ß√£o:** Objetivo: Realizar join externo (outer join).\n",
    "\n",
    "Instru√ß√µes: implemente usando fun√ß√µes do PySpark.\n",
    "\n",
    "**Starter code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162c8209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo inicial\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# TODO: completar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2deb49",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 34: Realizar join √† esquerda (left join).\n",
    "\n",
    "**Descri√ß√£o:** Objetivo: Realizar join √† esquerda (left join).\n",
    "\n",
    "Instru√ß√µes: implemente usando fun√ß√µes do PySpark.\n",
    "\n",
    "**Starter code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc968aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo inicial\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# TODO: completar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbbb7a4",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 35: Realizar join √† direita (right join).\n",
    "\n",
    "**Descri√ß√£o:** Objetivo: Realizar join √† direita (right join).\n",
    "\n",
    "Instru√ß√µes: implemente usando fun√ß√µes do PySpark.\n",
    "\n",
    "**Starter code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff73e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo inicial\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# TODO: completar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807dc3b5",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 36: Fazer join usando m√∫ltiplas chaves.\n",
    "\n",
    "**Descri√ß√£o:** Objetivo: Fazer join usando m√∫ltiplas chaves.\n",
    "\n",
    "Instru√ß√µes: implemente usando fun√ß√µes do PySpark.\n",
    "\n",
    "**Starter code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b54b3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo inicial\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# TODO: completar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6cceea",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 37: Usar union para concatenar DataFrames.\n",
    "\n",
    "**Descri√ß√£o:** Objetivo: Usar union para concatenar DataFrames.\n",
    "\n",
    "Instru√ß√µes: implemente usando fun√ß√µes do PySpark.\n",
    "\n",
    "**Starter code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ac5a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo inicial\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# TODO: completar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3458e2",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 38: Remover duplicados (dropDuplicates).\n",
    "\n",
    "**Descri√ß√£o:** Objetivo: Remover duplicados (dropDuplicates).\n",
    "\n",
    "Instru√ß√µes: implemente usando fun√ß√µes do PySpark.\n",
    "\n",
    "**Starter code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b63186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo inicial\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# TODO: completar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deac0b71",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 39: Criar colunas condicionais (when / otherwise).\n",
    "\n",
    "**Descri√ß√£o:** Objetivo: Criar colunas condicionais (when / otherwise).\n",
    "\n",
    "Instru√ß√µes: implemente usando fun√ß√µes do PySpark.\n",
    "\n",
    "**Starter code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adebd423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo inicial\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# TODO: completar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28fd0a7",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 40: Criar colunas baseadas em regex (regexp_extract).\n",
    "\n",
    "**Descri√ß√£o:** Objetivo: Criar colunas baseadas em regex (regexp_extract).\n",
    "\n",
    "Instru√ß√µes: implemente usando fun√ß√µes do PySpark.\n",
    "\n",
    "**Starter code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ae8106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo inicial\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# TODO: completar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6464cee4",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 41: Filtrar usando regex (rlike).\n",
    "\n",
    "**Descri√ß√£o:** Objetivo: Filtrar usando regex (rlike).\n",
    "\n",
    "Instru√ß√µes: implemente usando fun√ß√µes do PySpark.\n",
    "\n",
    "**Starter code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3f7c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo inicial\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# TODO: completar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eadb2e9",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 42: Explodir arrays (explode).\n",
    "\n",
    "**Descri√ß√£o:** Objetivo: Explodir arrays (explode).\n",
    "\n",
    "Instru√ß√µes: implemente usando fun√ß√µes do PySpark.\n",
    "\n",
    "**Starter code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0a445d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo inicial\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# TODO: completar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e16371",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 43: Agrupar dados e gerar listas (collect_list).\n",
    "\n",
    "**Descri√ß√£o:** Objetivo: Agrupar dados e gerar listas (collect_list).\n",
    "\n",
    "Instru√ß√µes: implemente usando fun√ß√µes do PySpark.\n",
    "\n",
    "**Starter code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a45e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo inicial\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# TODO: completar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57bd5cea",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 44: Transformar strings para mai√∫sculas/min√∫sculas.\n",
    "\n",
    "**Descri√ß√£o:** Objetivo: Transformar strings para mai√∫sculas/min√∫sculas.\n",
    "\n",
    "Instru√ß√µes: implemente usando fun√ß√µes do PySpark.\n",
    "\n",
    "**Starter code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3774f7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo inicial\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# TODO: completar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2cbb01",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 45: Extrair partes de datas (year, month, dayofmonth).\n",
    "\n",
    "**Descri√ß√£o:** Objetivo: Extrair partes de datas (year, month, dayofmonth).\n",
    "\n",
    "Instru√ß√µes: implemente usando fun√ß√µes do PySpark.\n",
    "\n",
    "**Starter code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72b56f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo inicial\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# TODO: completar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0b52d7",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 46: Calcular diferen√ßa entre datas (datediff).\n",
    "\n",
    "**Descri√ß√£o:** Objetivo: Calcular diferen√ßa entre datas (datediff).\n",
    "\n",
    "Instru√ß√µes: implemente usando fun√ß√µes do PySpark.\n",
    "\n",
    "**Starter code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff4d5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo inicial\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# TODO: completar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1128d255",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 47: Adicionar dias a uma data (date_add).\n",
    "\n",
    "**Descri√ß√£o:** Objetivo: Adicionar dias a uma data (date_add).\n",
    "\n",
    "Instru√ß√µes: implemente usando fun√ß√µes do PySpark.\n",
    "\n",
    "**Starter code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2721a617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo inicial\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# TODO: completar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdf34ed",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 48: Criar coluna de timestamp atual (current_timestamp).\n",
    "\n",
    "**Descri√ß√£o:** Objetivo: Criar coluna de timestamp atual (current_timestamp).\n",
    "\n",
    "Instru√ß√µes: implemente usando fun√ß√µes do PySpark.\n",
    "\n",
    "**Starter code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df245346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo inicial\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# TODO: completar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a49b94d",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 49: Fazer pivot de dados (pivot).\n",
    "\n",
    "**Descri√ß√£o:** Objetivo: Fazer pivot de dados (pivot).\n",
    "\n",
    "Instru√ß√µes: implemente usando fun√ß√µes do PySpark.\n",
    "\n",
    "**Starter code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0111a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo inicial\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# TODO: completar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e8e443",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 50: Fazer unpivot/melt (via manipula√ß√£o).\n",
    "\n",
    "**Descri√ß√£o:** Objetivo: Fazer unpivot/melt (via manipula√ß√£o).\n",
    "\n",
    "Instru√ß√µes: implemente usando fun√ß√µes do PySpark.\n",
    "\n",
    "**Starter code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57793fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo inicial\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# TODO: completar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c810e8d0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üü¶ Se√ß√£o: Avan√ßado\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6280693",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 51: Crear UDF (User Defined Function) simples.\n",
    "\n",
    "**Descri√ß√£o:** Objetivo: Crear UDF (User Defined Function) simples.\n",
    "\n",
    "Instru√ß√µes: implemente com foco em performance, escalabilidade e melhores pr√°ticas.\n",
    "\n",
    "**Starter code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abffc11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo inicial\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# TODO: completar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0405c0d8",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 52: Criar UDF para manipula√ß√£o de strings.\n",
    "\n",
    "**Descri√ß√£o:** Objetivo: Criar UDF para manipula√ß√£o de strings.\n",
    "\n",
    "Instru√ß√µes: implemente com foco em performance, escalabilidade e melhores pr√°ticas.\n",
    "\n",
    "**Starter code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0ec1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo inicial\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# TODO: completar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c637c25",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 53: Criar UDF com m√∫ltiplos par√¢metros.\n",
    "\n",
    "**Descri√ß√£o:** Objetivo: Criar UDF com m√∫ltiplos par√¢metros.\n",
    "\n",
    "Instru√ß√µes: implemente com foco em performance, escalabilidade e melhores pr√°ticas.\n",
    "\n",
    "**Starter code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d498711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo inicial\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# TODO: completar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb15daa",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 54: Criar UDF usando Pandas (pandas_udf).\n",
    "\n",
    "**Descri√ß√£o:** Objetivo: Criar UDF usando Pandas (pandas_udf).\n",
    "\n",
    "Instru√ß√µes: implemente com foco em performance, escalabilidade e melhores pr√°ticas.\n",
    "\n",
    "**Starter code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c099991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo inicial\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# TODO: completar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1999edcb",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 55: Usar fun√ß√µes SQL diretamente (spark.sql).\n",
    "\n",
    "**Descri√ß√£o:** Objetivo: Usar fun√ß√µes SQL diretamente (spark.sql).\n",
    "\n",
    "Instru√ß√µes: implemente com foco em performance, escalabilidade e melhores pr√°ticas.\n",
    "\n",
    "**Starter code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c4e2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo inicial\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# TODO: completar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960b8e53",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 56: Criar view tempor√°ria (createOrReplaceTempView).\n",
    "\n",
    "**Descri√ß√£o:** Objetivo: Criar view tempor√°ria (createOrReplaceTempView).\n",
    "\n",
    "Instru√ß√µes: implemente com foco em performance, escalabilidade e melhores pr√°ticas.\n",
    "\n",
    "**Starter code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c980a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo inicial\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# TODO: completar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef7f751",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 57: Criar view global (createGlobalTempView).\n",
    "\n",
    "**Descri√ß√£o:** Objetivo: Criar view global (createGlobalTempView).\n",
    "\n",
    "Instru√ß√µes: implemente com foco em performance, escalabilidade e melhores pr√°ticas.\n",
    "\n",
    "**Starter code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc504c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo inicial\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# TODO: completar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db8d823",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 58: Ler dados de um banco de dados via JDBC.\n",
    "\n",
    "**Descri√ß√£o:** Objetivo: Ler dados de um banco de dados via JDBC.\n",
    "\n",
    "Instru√ß√µes: implemente com foco em performance, escalabilidade e melhores pr√°ticas.\n",
    "\n",
    "**Starter code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3c48dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo inicial\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# TODO: completar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce1b58a",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 59: Salvar dados em um banco via JDBC.\n",
    "\n",
    "**Descri√ß√£o:** Objetivo: Salvar dados em um banco via JDBC.\n",
    "\n",
    "Instru√ß√µes: implemente com foco em performance, escalabilidade e melhores pr√°ticas.\n",
    "\n",
    "**Starter code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810a4126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo inicial\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# TODO: completar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525d54d2",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 60: Ler dados de um bucket S3.\n",
    "\n",
    "**Descri√ß√£o:** Objetivo: Ler dados de um bucket S3.\n",
    "\n",
    "Instru√ß√µes: implemente com foco em performance, escalabilidade e melhores pr√°ticas.\n",
    "\n",
    "**Starter code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb99a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo inicial\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# TODO: completar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f368999",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 61: Salvar dados em um bucket S3.\n",
    "\n",
    "**Descri√ß√£o:** Objetivo: Salvar dados em um bucket S3.\n",
    "\n",
    "Instru√ß√µes: implemente com foco em performance, escalabilidade e melhores pr√°ticas.\n",
    "\n",
    "**Starter code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f356ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo inicial\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# TODO: completar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bf314b",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 62: Ler dados do Azure Data Lake.\n",
    "\n",
    "**Descri√ß√£o:** Objetivo: Ler dados do Azure Data Lake.\n",
    "\n",
    "Instru√ß√µes: implemente com foco em performance, escalabilidade e melhores pr√°ticas.\n",
    "\n",
    "**Starter code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4401f4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo inicial\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# TODO: completar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84561916",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 63: Salvar dados no Azure Data Lake.\n",
    "\n",
    "**Descri√ß√£o:** Objetivo: Salvar dados no Azure Data Lake.\n",
    "\n",
    "Instru√ß√µes: implemente com foco em performance, escalabilidade e melhores pr√°ticas.\n",
    "\n",
    "**Starter code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699e16f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo inicial\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# TODO: completar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d638226c",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 64: Ler dados do Google Cloud Storage.\n",
    "\n",
    "**Descri√ß√£o:** Objetivo: Ler dados do Google Cloud Storage.\n",
    "\n",
    "Instru√ß√µes: implemente com foco em performance, escalabilidade e melhores pr√°ticas.\n",
    "\n",
    "**Starter code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94edcb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo inicial\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# TODO: completar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b3cf14",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 65: Usar broadcast join para otimiza√ß√£o.\n",
    "\n",
    "**Descri√ß√£o:** Objetivo: Usar broadcast join para otimiza√ß√£o.\n",
    "\n",
    "Instru√ß√µes: implemente com foco em performance, escalabilidade e melhores pr√°ticas.\n",
    "\n",
    "**Starter code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3c334f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo inicial\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# TODO: completar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c727ad7",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 66: Usar cache() para melhorar performance.\n",
    "\n",
    "**Descri√ß√£o:** Objetivo: Usar cache() para melhorar performance.\n",
    "\n",
    "Instru√ß√µes: implemente com foco em performance, escalabilidade e melhores pr√°ticas.\n",
    "\n",
    "**Starter code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46310635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo inicial\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# TODO: completar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fea3cdb",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 67: Usar persist() com diferentes n√≠veis de armazenamento.\n",
    "\n",
    "**Descri√ß√£o:** Objetivo: Usar persist() com diferentes n√≠veis de armazenamento.\n",
    "\n",
    "Instru√ß√µes: implemente com foco em performance, escalabilidade e melhores pr√°ticas.\n",
    "\n",
    "**Starter code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76fa0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo inicial\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# TODO: completar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e23e65a",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 68: Analisar plano de execu√ß√£o (explain).\n",
    "\n",
    "**Descri√ß√£o:** Objetivo: Analisar plano de execu√ß√£o (explain).\n",
    "\n",
    "Instru√ß√µes: implemente com foco em performance, escalabilidade e melhores pr√°ticas.\n",
    "\n",
    "**Starter code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d494100d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo inicial\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# TODO: completar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e57fed",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 69: Usar repartition() para balancear dados.\n",
    "\n",
    "**Descri√ß√£o:** Objetivo: Usar repartition() para balancear dados.\n",
    "\n",
    "Instru√ß√µes: implemente com foco em performance, escalabilidade e melhores pr√°ticas.\n",
    "\n",
    "**Starter code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d75f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo inicial\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# TODO: completar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62afb86",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 70: Usar coalesce() para reduzir parti√ß√µes.\n",
    "\n",
    "**Descri√ß√£o:** Objetivo: Usar coalesce() para reduzir parti√ß√µes.\n",
    "\n",
    "Instru√ß√µes: implemente com foco em performance, escalabilidade e melhores pr√°ticas.\n",
    "\n",
    "**Starter code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ca7f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo inicial\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# TODO: completar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789eafaa",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 71: Otimizar leitura de arquivos grandes (parquet).\n",
    "\n",
    "**Descri√ß√£o:** Objetivo: Otimizar leitura de arquivos grandes (parquet).\n",
    "\n",
    "Instru√ß√µes: implemente com foco em performance, escalabilidade e melhores pr√°ticas.\n",
    "\n",
    "**Starter code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7dddb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo inicial\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# TODO: completar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54dea7b",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 72: Criar colunas usando fun√ß√µes complexas (array, map).\n",
    "\n",
    "**Descri√ß√£o:** Objetivo: Criar colunas usando fun√ß√µes complexas (array, map).\n",
    "\n",
    "Instru√ß√µes: implemente com foco em performance, escalabilidade e melhores pr√°ticas.\n",
    "\n",
    "**Starter code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810026dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo inicial\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# TODO: completar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8170b2e",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 73: Filtrar usando fun√ß√µes de array (array_contains).\n",
    "\n",
    "**Descri√ß√£o:** Objetivo: Filtrar usando fun√ß√µes de array (array_contains).\n",
    "\n",
    "Instru√ß√µes: implemente com foco em performance, escalabilidade e melhores pr√°ticas.\n",
    "\n",
    "**Starter code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b152e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo inicial\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# TODO: completar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10c75d9",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 74: Trabalhar com colunas do tipo struct.\n",
    "\n",
    "**Descri√ß√£o:** Objetivo: Trabalhar com colunas do tipo struct.\n",
    "\n",
    "Instru√ß√µes: implemente com foco em performance, escalabilidade e melhores pr√°ticas.\n",
    "\n",
    "**Starter code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42313545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo inicial\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# TODO: completar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691d9e34",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 75: Criar coluna JSON a partir de struct.\n",
    "\n",
    "**Descri√ß√£o:** Objetivo: Criar coluna JSON a partir de struct.\n",
    "\n",
    "Instru√ß√µes: implemente com foco em performance, escalabilidade e melhores pr√°ticas.\n",
    "\n",
    "**Starter code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe62dbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo inicial\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# TODO: completar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b5f2d7",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 76: Ler coluna JSON e transform√°-la em colunas (from_json).\n",
    "\n",
    "**Descri√ß√£o:** Objetivo: Ler coluna JSON e transform√°-la em colunas (from_json).\n",
    "\n",
    "Instru√ß√µes: implemente com foco em performance, escalabilidade e melhores pr√°ticas.\n",
    "\n",
    "**Starter code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12b3816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo inicial\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# TODO: completar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b166a5",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 77: Serializar colunas para JSON (to_json).\n",
    "\n",
    "**Descri√ß√£o:** Objetivo: Serializar colunas para JSON (to_json).\n",
    "\n",
    "Instru√ß√µes: implemente com foco em performance, escalabilidade e melhores pr√°ticas.\n",
    "\n",
    "**Starter code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733332f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo inicial\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# TODO: completar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffd85a2",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 78: Criar coluna de hash (sha2).\n",
    "\n",
    "**Descri√ß√£o:** Objetivo: Criar coluna de hash (sha2).\n",
    "\n",
    "Instru√ß√µes: implemente com foco em performance, escalabilidade e melhores pr√°ticas.\n",
    "\n",
    "**Starter code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3eedb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo inicial\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# TODO: completar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773c2f6c",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 79: Criar coluna UUID.\n",
    "\n",
    "**Descri√ß√£o:** Objetivo: Criar coluna UUID.\n",
    "\n",
    "Instru√ß√µes: implemente com foco em performance, escalabilidade e melhores pr√°ticas.\n",
    "\n",
    "**Starter code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d294ec86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo inicial\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# TODO: completar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce96e70b",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 80: Detectar outliers usando fun√ß√µes estat√≠sticas.\n",
    "\n",
    "**Descri√ß√£o:** Objetivo: Detectar outliers usando fun√ß√µes estat√≠sticas.\n",
    "\n",
    "Instru√ß√µes: implemente com foco em performance, escalabilidade e melhores pr√°ticas.\n",
    "\n",
    "**Starter code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fc12e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo inicial\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# TODO: completar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11703c5c",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 81: Criar agrega√ß√µes com m√∫ltiplas fun√ß√µes simult√¢neas.\n",
    "\n",
    "**Descri√ß√£o:** Objetivo: Criar agrega√ß√µes com m√∫ltiplas fun√ß√µes simult√¢neas.\n",
    "\n",
    "Instru√ß√µes: implemente com foco em performance, escalabilidade e melhores pr√°ticas.\n",
    "\n",
    "**Starter code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8b6636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo inicial\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# TODO: completar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4187d7d",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 82: Ordenar dados dentro de cada grupo (Window).\n",
    "\n",
    "**Descri√ß√£o:** Objetivo: Ordenar dados dentro de cada grupo (Window).\n",
    "\n",
    "Instru√ß√µes: implemente com foco em performance, escalabilidade e melhores pr√°ticas.\n",
    "\n",
    "**Starter code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa533904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo inicial\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# TODO: completar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6034d51",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 83: Criar ranking denso (dense_rank).\n",
    "\n",
    "**Descri√ß√£o:** Objetivo: Criar ranking denso (dense_rank).\n",
    "\n",
    "Instru√ß√µes: implemente com foco em performance, escalabilidade e melhores pr√°ticas.\n",
    "\n",
    "**Starter code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea373a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo inicial\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# TODO: completar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a220f22",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 84: Usar fun√ß√µes cumulativas (sum().over()).\n",
    "\n",
    "**Descri√ß√£o:** Objetivo: Usar fun√ß√µes cumulativas (sum().over()).\n",
    "\n",
    "Instru√ß√µes: implemente com foco em performance, escalabilidade e melhores pr√°ticas.\n",
    "\n",
    "**Starter code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96078562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo inicial\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# TODO: completar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a855dbd",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 85: Criar coluna categ√≥rica baseada em ranges de valores.\n",
    "\n",
    "**Descri√ß√£o:** Objetivo: Criar coluna categ√≥rica baseada em ranges de valores.\n",
    "\n",
    "Instru√ß√µes: implemente com foco em performance, escalabilidade e melhores pr√°ticas.\n",
    "\n",
    "**Starter code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671cb57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo inicial\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# TODO: completar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937d5c3d",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 86: Trabalhar com dados semi-estruturados (JSON/Avro).\n",
    "\n",
    "**Descri√ß√£o:** Objetivo: Trabalhar com dados semi-estruturados (JSON/Avro).\n",
    "\n",
    "Instru√ß√µes: implemente com foco em performance, escalabilidade e melhores pr√°ticas.\n",
    "\n",
    "**Starter code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbecbf40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo inicial\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# TODO: completar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e348f14",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 87: Ler dados compactados (gzip).\n",
    "\n",
    "**Descri√ß√£o:** Objetivo: Ler dados compactados (gzip).\n",
    "\n",
    "Instru√ß√µes: implemente com foco em performance, escalabilidade e melhores pr√°ticas.\n",
    "\n",
    "**Starter code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f06641e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo inicial\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# TODO: completar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a183a13f",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 88: Escrever dados compactados (gzip, snappy).\n",
    "\n",
    "**Descri√ß√£o:** Objetivo: Escrever dados compactados (gzip, snappy).\n",
    "\n",
    "Instru√ß√µes: implemente com foco em performance, escalabilidade e melhores pr√°ticas.\n",
    "\n",
    "**Starter code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0319767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo inicial\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# TODO: completar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e176ef",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 89: Criar DataFrame a partir de RDD.\n",
    "\n",
    "**Descri√ß√£o:** Objetivo: Criar DataFrame a partir de RDD.\n",
    "\n",
    "Instru√ß√µes: implemente com foco em performance, escalabilidade e melhores pr√°ticas.\n",
    "\n",
    "**Starter code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72cc1b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo inicial\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# TODO: completar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa9f039",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 90: Converter DataFrame para RDD e vice-versa.\n",
    "\n",
    "**Descri√ß√£o:** Objetivo: Converter DataFrame para RDD e vice-versa.\n",
    "\n",
    "Instru√ß√µes: implemente com foco em performance, escalabilidade e melhores pr√°ticas.\n",
    "\n",
    "**Starter code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52e5709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo inicial\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# TODO: completar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a07ee50",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 91: Usar mapPartitions para processar dados em lote.\n",
    "\n",
    "**Descri√ß√£o:** Objetivo: Usar mapPartitions para processar dados em lote.\n",
    "\n",
    "Instru√ß√µes: implemente com foco em performance, escalabilidade e melhores pr√°ticas.\n",
    "\n",
    "**Starter code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e192803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo inicial\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# TODO: completar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5280e7",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 92: Usar foreachPartition para a√ß√µes por parti√ß√£o.\n",
    "\n",
    "**Descri√ß√£o:** Objetivo: Usar foreachPartition para a√ß√µes por parti√ß√£o.\n",
    "\n",
    "Instru√ß√µes: implemente com foco em performance, escalabilidade e melhores pr√°ticas.\n",
    "\n",
    "**Starter code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c6322a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo inicial\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# TODO: completar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136495ec",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 93: Implementar l√≥gica de ETL completo (ingest√£o ‚Üí transforma√ß√£o ‚Üí sa√≠da).\n",
    "\n",
    "**Descri√ß√£o:** Objetivo: Implementar l√≥gica de ETL completo (ingest√£o ‚Üí transforma√ß√£o ‚Üí sa√≠da).\n",
    "\n",
    "Instru√ß√µes: implemente com foco em performance, escalabilidade e melhores pr√°ticas.\n",
    "\n",
    "**Starter code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b6d6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo inicial\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# TODO: completar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbd30b9",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 94: Integrar PySpark com Delta Lake.\n",
    "\n",
    "**Descri√ß√£o:** Objetivo: Integrar PySpark com Delta Lake.\n",
    "\n",
    "Instru√ß√µes: implemente com foco em performance, escalabilidade e melhores pr√°ticas.\n",
    "\n",
    "**Starter code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4dde33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo inicial\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# TODO: completar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c96f439",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 95: Criar e ler tabelas Delta.\n",
    "\n",
    "**Descri√ß√£o:** Objetivo: Criar e ler tabelas Delta.\n",
    "\n",
    "Instru√ß√µes: implemente com foco em performance, escalabilidade e melhores pr√°ticas.\n",
    "\n",
    "**Starter code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e4c1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo inicial\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# TODO: completar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbc8303",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 96: Usar merge no Delta Lake (UPSERT).\n",
    "\n",
    "**Descri√ß√£o:** Objetivo: Usar merge no Delta Lake (UPSERT).\n",
    "\n",
    "Instru√ß√µes: implemente com foco em performance, escalabilidade e melhores pr√°ticas.\n",
    "\n",
    "**Starter code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a595e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo inicial\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# TODO: completar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54c6573",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 97: Usar Time Travel no Delta Lake.\n",
    "\n",
    "**Descri√ß√£o:** Objetivo: Usar Time Travel no Delta Lake.\n",
    "\n",
    "Instru√ß√µes: implemente com foco em performance, escalabilidade e melhores pr√°ticas.\n",
    "\n",
    "**Starter code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa2f39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo inicial\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# TODO: completar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901f5afa",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 98: Usar Change Data Feed (Delta Lake).\n",
    "\n",
    "**Descri√ß√£o:** Objetivo: Usar Change Data Feed (Delta Lake).\n",
    "\n",
    "Instru√ß√µes: implemente com foco em performance, escalabilidade e melhores pr√°ticas.\n",
    "\n",
    "**Starter code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1dcecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo inicial\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# TODO: completar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64476b18",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 99: Criar particionamento por coluna ao salvar dados.\n",
    "\n",
    "**Descri√ß√£o:** Objetivo: Criar particionamento por coluna ao salvar dados.\n",
    "\n",
    "Instru√ß√µes: implemente com foco em performance, escalabilidade e melhores pr√°ticas.\n",
    "\n",
    "**Starter code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8855aa6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo inicial\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# TODO: completar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e62c6c",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 100: Criar bucketiza√ß√£o de dados ao salvar.\n",
    "\n",
    "**Descri√ß√£o:** Objetivo: Criar bucketiza√ß√£o de dados ao salvar.\n",
    "\n",
    "Instru√ß√µes: implemente com foco em performance, escalabilidade e melhores pr√°ticas.\n",
    "\n",
    "**Starter code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be3c763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo inicial\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# TODO: completar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bef0942",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 101: Trabalhar com arquivos ORC.\n",
    "\n",
    "**Descri√ß√£o:** Objetivo: Trabalhar com arquivos ORC.\n",
    "\n",
    "Instru√ß√µes: implemente com foco em performance, escalabilidade e melhores pr√°ticas.\n",
    "\n",
    "**Starter code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c9cdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo inicial\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# TODO: completar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc76532",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 102: Escrever consultas SQL otimizadas no Spark SQL.\n",
    "\n",
    "**Descri√ß√£o:** Objetivo: Escrever consultas SQL otimizadas no Spark SQL.\n",
    "\n",
    "Instru√ß√µes: implemente com foco em performance, escalabilidade e melhores pr√°ticas.\n",
    "\n",
    "**Starter code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c5e1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo inicial\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# TODO: completar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca5a654",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 103: Criar pipeline de dados no PySpark integrado ao Airflow.\n",
    "\n",
    "**Descri√ß√£o:** Objetivo: Criar pipeline de dados no PySpark integrado ao Airflow.\n",
    "\n",
    "Instru√ß√µes: implemente com foco em performance, escalabilidade e melhores pr√°ticas.\n",
    "\n",
    "**Starter code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0100752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo inicial\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# TODO: completar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5003563",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 104: Ler streaming de dados de um diret√≥rio.\n",
    "\n",
    "**Descri√ß√£o:** Objetivo: Ler streaming de dados de um diret√≥rio.\n",
    "\n",
    "Instru√ß√µes: implemente com foco em performance, escalabilidade e melhores pr√°ticas.\n",
    "\n",
    "**Starter code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90973b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo inicial\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# TODO: completar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64141e29",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 105: Escrever streaming para Kafka.\n",
    "\n",
    "**Descri√ß√£o:** Objetivo: Escrever streaming para Kafka.\n",
    "\n",
    "Instru√ß√µes: implemente com foco em performance, escalabilidade e melhores pr√°ticas.\n",
    "\n",
    "**Starter code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01e5811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo inicial\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# TODO: completar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e6e6a9",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 106: Ler streaming do Kafka.\n",
    "\n",
    "**Descri√ß√£o:** Objetivo: Ler streaming do Kafka.\n",
    "\n",
    "Instru√ß√µes: implemente com foco em performance, escalabilidade e melhores pr√°ticas.\n",
    "\n",
    "**Starter code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b95b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo inicial\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# TODO: completar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9194645",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 107: Criar agrega√ß√µes em streaming.\n",
    "\n",
    "**Descri√ß√£o:** Objetivo: Criar agrega√ß√µes em streaming.\n",
    "\n",
    "Instru√ß√µes: implemente com foco em performance, escalabilidade e melhores pr√°ticas.\n",
    "\n",
    "**Starter code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1c0b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo inicial\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# TODO: completar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075538ba",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 108: Fazer checkpoint de streaming.\n",
    "\n",
    "**Descri√ß√£o:** Objetivo: Fazer checkpoint de streaming.\n",
    "\n",
    "Instru√ß√µes: implemente com foco em performance, escalabilidade e melhores pr√°ticas.\n",
    "\n",
    "**Starter code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29dc9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo inicial\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# TODO: completar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a67f70",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 109: Criar alerta quando dado de streaming atende condi√ß√£o.\n",
    "\n",
    "**Descri√ß√£o:** Objetivo: Criar alerta quando dado de streaming atende condi√ß√£o.\n",
    "\n",
    "Instru√ß√µes: implemente com foco em performance, escalabilidade e melhores pr√°ticas.\n",
    "\n",
    "**Starter code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bb2269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo inicial\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# TODO: completar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf9f378",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 110: Integrar PySpark com MLlib para regress√£o linear.\n",
    "\n",
    "**Descri√ß√£o:** Objetivo: Integrar PySpark com MLlib para regress√£o linear.\n",
    "\n",
    "Instru√ß√µes: implemente com foco em performance, escalabilidade e melhores pr√°ticas.\n",
    "\n",
    "**Starter code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e8d3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo inicial\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# TODO: completar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd331e4",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 111: Treinar modelo de classifica√ß√£o com MLlib.\n",
    "\n",
    "**Descri√ß√£o:** Objetivo: Treinar modelo de classifica√ß√£o com MLlib.\n",
    "\n",
    "Instru√ß√µes: implemente com foco em performance, escalabilidade e melhores pr√°ticas.\n",
    "\n",
    "**Starter code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a76dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo inicial\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# TODO: completar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c03bda",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 112: Fazer previs√£o com modelo treinado.\n",
    "\n",
    "**Descri√ß√£o:** Objetivo: Fazer previs√£o com modelo treinado.\n",
    "\n",
    "Instru√ß√µes: implemente com foco em performance, escalabilidade e melhores pr√°ticas.\n",
    "\n",
    "**Starter code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0a5e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo inicial\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# TODO: completar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148272af",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 113: Avaliar modelo usando m√©tricas (BinaryClassificationEvaluator).\n",
    "\n",
    "**Descri√ß√£o:** Objetivo: Avaliar modelo usando m√©tricas (BinaryClassificationEvaluator).\n",
    "\n",
    "Instru√ß√µes: implemente com foco em performance, escalabilidade e melhores pr√°ticas.\n",
    "\n",
    "**Starter code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbfc29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo inicial\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# TODO: completar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b126ded6",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 114: Criar pipeline de ML com stages no PySpark.\n",
    "\n",
    "**Descri√ß√£o:** Objetivo: Criar pipeline de ML com stages no PySpark.\n",
    "\n",
    "Instru√ß√µes: implemente com foco em performance, escalabilidade e melhores pr√°ticas.\n",
    "\n",
    "**Starter code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885814be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo inicial\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# TODO: completar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06459718",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 115: Salvar e carregar modelo treinado.\n",
    "\n",
    "**Descri√ß√£o:** Objetivo: Salvar e carregar modelo treinado.\n",
    "\n",
    "Instru√ß√µes: implemente com foco em performance, escalabilidade e melhores pr√°ticas.\n",
    "\n",
    "**Starter code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093b1151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo inicial\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# TODO: completar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f104a7bd",
   "metadata": {},
   "source": [
    "# Processamento de Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d70586",
   "metadata": {},
   "source": [
    "##### Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e300a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x0000013A7F8FC1A0>\n",
      "3.5.4\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName (\"ReadFile\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(spark)\n",
    "print(spark.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e70046",
   "metadata": {},
   "source": [
    "##### Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8a4fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"http://127.0.0.1:8000/dados\"\n",
    "\n",
    "# Enviar a API Key no header\n",
    "headers = {\n",
    "    \"X-API-KEY\": \"minha_chave_super_secreta\"\n",
    "}\n",
    "\n",
    "response = requests.get(url, headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fa564384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+--------------------+--------------------+---------+--------------------+--------------------+--------------------+----+--------------------+\n",
      "|id_empregado|        primeiroNome|            nomeMeio|          ultimoNome|NameStyle|      dataNascimento|               Email|            Telefone|Sexo|        Departamento|\n",
      "+------------+--------------------+--------------------+--------------------+---------+--------------------+--------------------+--------------------+----+--------------------+\n",
      "|           1|Guy              ...|R                ...|Gilbert          ...|        N|1981/11/12 00:00:...|guy1@adventure-wo...|320-555-0195     ...|   M|Production       ...|\n",
      "|           2|Kevin            ...|F                ...|Brown            ...|        N|1986/12/01 00:00:...|kevin0@adventure-...|150-555-0189     ...|   M|Marketing        ...|\n",
      "|           3|Roberto          ...|                 ...|Tamburello       ...|        N|1974/06/12 00:00:...|roberto0@adventur...|212-555-0187     ...|   M|Engineering      ...|\n",
      "|           4|Rob              ...|                 ...|Walters          ...|        N|1974/07/23 00:00:...|rob0@adventure-wo...|612-555-0100     ...|   M|Tool Design      ...|\n",
      "|           5|Rob              ...|                 ...|Walters          ...|        N|1974/07/23 00:00:...|rob0@adventure-wo...|612-555-0100     ...|   M|Tool Design      ...|\n",
      "|           6|Thierry          ...|B                ...|D'Hers           ...|        N|1959/02/26 00:00:...|thierry0@adventur...|168-555-0183     ...|   M|Tool Design      ...|\n",
      "|           7|David            ...|M                ...|Bradley          ...|        N|1974/10/17 00:00:...|david0@adventure-...|913-555-0172     ...|   M|Marketing        ...|\n",
      "|           8|David            ...|M                ...|Bradley          ...|        N|1974/10/17 00:00:...|david0@adventure-...|913-555-0172     ...|   M|Marketing        ...|\n",
      "|           9|JoLynn           ...|M                ...|Dobney           ...|        N|1955/08/16 00:00:...|jolynn0@adventure...|903-555-0145     ...|   F|Production       ...|\n",
      "|          10|Ruth             ...|Ann              ...|Ellerbrock       ...|        N|1956/01/03 00:00:...|ruth0@adventure-w...|145-555-0130     ...|   F|Production       ...|\n",
      "|          11|Gail             ...|A                ...|Erickson         ...|        N|1952/04/27 00:00:...|gail0@adventure-w...|849-555-0139     ...|   F|Engineering      ...|\n",
      "|          12|Barry            ...|K                ...|Johnson          ...|        N|1955/10/25 00:00:...|barry0@adventure-...|206-555-0180     ...|   M|Production       ...|\n",
      "|          13|Jossef           ...|H                ...|Goldberg         ...|        N|1958/10/09 00:00:...|jossef0@adventure...|122-555-0189     ...|   M|Engineering      ...|\n",
      "|          14|Terri            ...|Lee              ...|Duffy            ...|        N|1971/03/01 00:00:...|terri0@adventure-...|819-555-0175     ...|   F|Engineering      ...|\n",
      "|          15|Sidney           ...|M                ...|Higa             ...|        N|1956/03/30 00:00:...|sidney0@adventure...|424-555-0189     ...|   M|Production       ...|\n",
      "|          16|Taylor           ...|R                ...|Maxwell          ...|        N|1955/10/31 00:00:...|taylor0@adventure...|508-555-0165     ...|   M|Production       ...|\n",
      "|          17|Jeffrey          ...|L                ...|Ford             ...|        N|1956/02/09 00:00:...|jeffrey0@adventur...|984-555-0185     ...|   M|Production       ...|\n",
      "|          18|Jo               ...|A                ...|Brown            ...|        N|1956/05/08 00:00:...|jo0@adventure-wor...|632-555-0129     ...|   F|Production       ...|\n",
      "|          19|Doris            ...|M                ...|Hartwig          ...|        N|1955/11/03 00:00:...|doris0@adventure-...|328-555-0150     ...|   F|Production       ...|\n",
      "|          20|John             ...|T                ...|Campbell         ...|        N|1956/03/07 00:00:...|john0@adventure-w...|435-555-0113     ...|   M|Production       ...|\n",
      "+------------+--------------------+--------------------+--------------------+---------+--------------------+--------------------+--------------------+----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read \\\n",
    "    .option(\"header\", True) \\\n",
    "    .option(\"delimiter\", \";\") \\\n",
    "    .option (\"inferSchema\", True) \\\n",
    "    .option(\"encoding\", \"UTF-8\") \\\n",
    "    .csv(\"colaboradores.csv\")\n",
    " # ou \"ISO-8859-1\", \"latin1\"\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdea485a",
   "metadata": {},
   "source": [
    "##### Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e18132",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
